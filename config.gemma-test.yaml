# Turbo-Heartbeat Configuration
# Generated by Siegfried on 2026-02-07
# DO NOT EDIT MANUALLY — managed by assistant
#
# Uses OpenAI-compatible /v1/chat/completions API.
# Works with: Ollama, LM Studio, llama.cpp, vLLM, LocalAI, Jan,
#             Groq, Mistral, OpenRouter, OpenAI, and any compatible endpoint.

version: "1.0"
profile: "A"

# Triage Model — just a URL, a key, and a model name
api_base: "http://localhost:11434/v1"
api_key: "ollama"
model: "gemma3:4b"
# Set true for thinking models (GLM, Qwen-thinking, DeepSeek-R1) on local engines
# Adds "think":false to suppress wasted reasoning tokens
# Cloud providers (Groq, OpenAI) reject this field — leave false for them
disable_thinking: false

# Timing
interval_seconds: 30
cooldown_seconds: 300
max_per_hour: 6

# Ollama keep_alive — how long Ollama keeps the model in RAM after a request.
# Set to "auto" (default) to calculate: interval + 120s buffer.
# Prevents cold-start latency spikes when interval >= 4 minutes.
# Only applies to Ollama; ignored by other providers.
# Format: seconds (int) or "auto"
keep_alive: "auto"

# Quiet Hours
quiet_start: "23:00"
quiet_end: "08:00"
timezone: "Europe/Berlin"

# Signal Collectors
signals_email: true
signals_calendar: true
signals_system: true

# Credentials
email_credentials: "/home/alf/.config/siegfried/email_credentials.json"

# Critical Notification
notify_channel: "telegram"
notify_email: "alfred@mickautsch.de"
smtp_host: "smtp.strato.de"
smtp_port: 465
smtp_user: "siegfried@mickautsch.de"
smtp_pass: ""
smtp_from: "siegfried@mickautsch.de"

# Gateway (for escalation)
gateway_port: 18789
gateway_token: "33319402a20851fe8a077f8cc1b46ea4e25f7acefca5166a"

# Statistics
stats_enabled: true
stats_retention_days: 30
